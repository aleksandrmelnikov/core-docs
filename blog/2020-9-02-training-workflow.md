---
id: object-detection-detr
title: Detection Transformer on Onepanel
author: Savan Visalpara
author_title: Machine Learning Engineer Intern
author_url: https://github.com/savan77
author_image_url: http://1.gravatar.com/avatar/0d1dd8e9d8d8c1198781c5fe8af36184
tags: []
---


## Goal of the blog
Object Detection models are one of the most widely used models among other computer vision tasks. Consequently, there is a lot of research going on in this area. One of the most popular model for object detection is Faster R-CNN. Faster R-CNN builds on top of Fast R-CNN and R-CNN to further improve the speed and accuracy of the model. However, Faster RCNN consists of multiple components which makes it non end-to-end model. By now, it has been well established that end-to-end models make a lot of things easier and in fact it has led to some great results in NLP tasks such as machine translation. In this blog, we are going to see how DEtection TRansformer provides a great alternative to Faster R-CNN as an end-to-end model and how you can use it on Onepanel.


## Why DETR
This paper isn't the first one to propose an end-to-end model for object detection models. Previous approaches <reference> used sequential models such as recurrent neural networks to predict bounding boxes, but the result wasn't on par with state-of-the-art models. Of couse, we can use conventional fully connected network for a fixed set of boxes, but it isn't usually the case. And to address the issue of permutation-invariance they used bipartite matching loss.

DETR uses bipartite matching loss as well, but turns to Transformers with parallel decoding instead of recurrent neural networks. Below image shows the architecture of DEtection Transformer.

<image-detr-model>

As the paper<link> mentions, DETR views object detection as a direct set prediction problem. Here is a brief summary of how it works.

1. Pass the image through a pre-trained Convolutional Neural Network (i.e ResNet 50) to generate the feature map.
2. Pass the feature map though a Transformer.
3. The output from Transformer's decoder is then passed through a feed forward network to generate final predicts
3. For training, we additionally pass this output through a loss function which performs bipartite matching between predicted bounding boxes and ground-truth boxes.

The first step is straight forward. They use ResNet-50 and ResNet-101 pre-trained on ImageNet to generate the feature maps. This can be achieved in few lines of code using `torchvision`. Since the detailed explanation of Transformer is beyond the scope of this blog, following sections attempts to explain it briefly. For more information on Transformer, check out this excellent blog post (http://jalammar.github.io/illustrated-transformer/) by Jay Alammar on Transformer.

## Transformer
Transformer is an encoder-decoder based architecture which leverages self-attention layers to gather information from the whole sequence. Transformers have gained a lot of popularity lately and they are being used in many state-of-the-art models for NLP tasks such as machine translation.

For DETR, this encoder takes in feature map combined with positional encoding as an input. Positional encoding allows Transformer to know the order of a word since, unlike RNN, it does not accept word sequentially and hence does not know the order of input words. Typically, positional encoding can be achieved by summing word embedding and positional embedding. Positional embeddings can be generated by repeating a pair of sines and cosines over time. This post(https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) provides a great explanation of position encoding in Transformers.

Combing back to encoder, the encoder block then uses 1 x 1 convolutional kernel to reduce the dimensionality of the feature map. Using 1 x 1 kernel we can essentially control the number of feature maps (or depth) without touching the size of input. Then, the input is flattened and passed through multiple encoders. Here, encoders follow the standard structure. That is, each encoder has a self-attention layer followed by a feed forward network. 

The decoding part is also very similar to the standard architecture with the major difference being the parallel decoding. DETR decodes, let's say, N objects in parallel contrary to standard approach where model like RNN is used to make prediction one time step at a time. A decoder has a self-attention layer, encoder-decoder attention, and a feed forward network. The encoder-decoder attention helps decoder focus on the relevant part of the input. 

Finally, the final output is computed by feed forward network and a linear projection layer. The FFN outputs the normalized center of a bounding box along with height and width of a bounding box, whereas linear project layer predicts the class label using softmax function. 

An important thing to note here is that since the model predicts a fix set of N objects, where N is way larger than the number of objects in ground-truth data, the author used a special class to represent 'no object was detected in this slot'. 


## Loss
Since the number of predicted objects is much larger than the objects in ground-truth data, they pad a vector representing ground-truth data with nulls to represent "no object". Using pair-wise matching cost, predicted boxes are then matched with target box such that the cost is minimum. As the author says, this process is similar to matching anchors to ground-truth objects in models such as SSD. 

<loss function>

The loss function used here is negative log-likelyhood for class label and a box. For box loss, a combination of l1 loss and IoU loss is used to ensure loss is scale-invariant since there could be small and big boxes.

## Using it on Onepanel
Now that we have some understanding of how DEtection Transformer works, let's see how can we use it on Onepanel. In the following section, we'll see how east it is to train this DETR model directly from CVAT with just a few clicks.

## References