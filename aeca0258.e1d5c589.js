(window.webpackJsonp=window.webpackJsonp||[]).push([[34],{135:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return s})),n.d(t,"rightToc",(function(){return c})),n.d(t,"default",(function(){return u}));var o=n(2),a=n(6),r=(n(0),n(156)),i={title:"Creating annotation models on CVAT",sidebar_label:"Create annotation model",description:"Onepanel use case - computer vision automatic annotation"},s={id:"getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model",title:"Creating annotation models on CVAT",description:"Onepanel use case - computer vision automatic annotation",source:"@site/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model.md",permalink:"/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model",editUrl:"https://github.com/onepanelio/core-docs/tree/master/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model.md",sidebar_label:"Create annotation model",sidebar:"gettingStarted",previous:{title:"Semi-automatic annotation with CVAT",permalink:"/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_automatic_annotation"},next:{title:"Adding custom deep learning model Workflow to CVAT",permalink:"/docs/getting-started/use-cases/computervision/annotation/cvat/adding_custom_model"}},c=[{value:"Why pre-annotate?",id:"why-pre-annotate",children:[]},{value:"Training deep learning model through CVAT",id:"training-deep-learning-model-through-cvat",children:[]},{value:"TensorFlow Object Detection API",id:"tensorflow-object-detection-api",children:[{value:"Hyperparameters",id:"hyperparameters",children:[]},{value:"Choosing the right base model",id:"choosing-the-right-base-model",children:[]}]},{value:"Training MaskRCNN model through CVAT",id:"training-maskrcnn-model-through-cvat",children:[]},{value:"How to run inference on test data using trained model",id:"how-to-run-inference-on-test-data-using-trained-model",children:[]},{value:"Notes",id:"notes",children:[]}],l={rightToc:c};function u(e){var t=e.components,n=Object(a.a)(e,["components"]);return Object(r.b)("wrapper",Object(o.a)({},l,n,{components:t,mdxType:"MDXLayout"}),Object(r.b)("h2",{id:"why-pre-annotate"},"Why pre-annotate?"),Object(r.b)("p",null,"Pre-annotation will cut the time to annotate large amounts of data by orders of magnitude. The idea is simple, annotate once then QC each successive dataset after."),Object(r.b)("p",null,"Once you have annotated enough data, you can train a model to pre-annotate the rest of your images with a few button clicks."),Object(r.b)("h2",{id:"training-deep-learning-model-through-cvat"},"Training deep learning model through CVAT"),Object(r.b)("p",null,Object(r.b)("img",Object(o.a)({parentName:"p"},{src:"/img/auto-annotation-v.2.0.png",alt:"CVAT flowchart"}))),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"Annotate enough images in your CVAT task.  "),Object(r.b)("li",{parentName:"ol"},"Go back to your CVAT dashboard and click on Actions and find ",Object(r.b)("inlineCode",{parentName:"li"},"Execute training workflow")," in that task. You will see a popup with a few options.  "),Object(r.b)("li",{parentName:"ol"},"Select the appropriate model type (TensorFlow OD API or MaskRCNN, you can add your own model as well) and then select the model (i.e ssd-mobilenet-v2-coco).  "),Object(r.b)("li",{parentName:"ol"},"Select the machine type. A machine with multiple GPUs will speed up your training process.  "),Object(r.b)("li",{parentName:"ol"},"Select the checkpoint model to start training from. These are the models that you trained previously in this namespace. This is optional. By default, a model trained on COCO will be used. Please make sure that the base model you select is compatible with the current task. The number of classes should be same, and if you use model trained on other types of data then accuracy might deteriorate."),Object(r.b)("li",{parentName:"ol"},"Enter right hyperparameters. See below for more details on hyperparameters.  "),Object(r.b)("li",{parentName:"ol"},"Your trained model, checkpoints, and classes file will be stored on your cloud storage (i.e s3 bucket).")),Object(r.b)("h2",{id:"tensorflow-object-detection-api"},"TensorFlow Object Detection API"),Object(r.b)("p",null,"You can use any of the models that we support for TensorFlow Object Detection API to train your custom pre-annotation models. Here, we provide a brief explanation on how to choose one model over another based on your needs. Some models are faster than others, whereas some are more accurate than others.  We hope this information will help you choose the right model for your task. "),Object(r.b)("p",null,Object(r.b)("img",Object(o.a)({parentName:"p"},{src:"/img/tf-object-detection.png",alt:"TensorFlow Object Detection Workflow"}))),Object(r.b)("h3",{id:"hyperparameters"},"Hyperparameters"),Object(r.b)("p",null,"You can specify some arguments in the ",Object(r.b)("inlineCode",{parentName:"p"},"Hyperparameters")," field seperated by new line. "),Object(r.b)("p",null,"Here is a sample for Tensorflow Object Detection API: "),Object(r.b)("pre",null,Object(r.b)("code",Object(o.a)({parentName:"pre"},{className:"language-bash"}),"num-steps=100\n")),Object(r.b)("p",null,"Details:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"num-steps : number of steps to train your model for. By default, we will train for an appropriate number of epochs depending upon the model."),Object(r.b)("li",{parentName:"ul"},"batch_size : batch size for the training"),Object(r.b)("li",{parentName:"ul"},"initial_learning_rate : initial learning rate for the model. We recommend you do not change this."),Object(r.b)("li",{parentName:"ul"},"num-clones (default=1): number of GPUs to train the model\xa0"),Object(r.b)("li",{parentName:"ul"},"schedule-step-1: step 1 for linear learning rate decay"),Object(r.b)("li",{parentName:"ul"},"schedule-step-2: step 2 for linear learning rate decay")),Object(r.b)("p",null,"Please note that number of classes will be automatically populated if you have ",Object(r.b)("inlineCode",{parentName:"p"},"sys-num-classes")," parameter defined in a workflow. Also, if you select a Machine type with 4 GPUs (Tesla V100), the following command can be used:\n",Object(r.b)("inlineCode",{parentName:"p"},"num_clones=4")),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Note that num_clones is 4 because there are 4 GPUs available.")),Object(r.b)("h3",{id:"choosing-the-right-base-model"},"Choosing the right base model"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"We currently support several faster-rcnn models. All of these models are similar except that of the backbone used for the feature extraction. The backbones used are, in increasing order of complexity (i.e more layers), ResNet50, ResNet101, InceptionResNetV2. As the model complexity increases the computation requirement will also increase. If you have very complicated data (i.e hundreds of annotations in one image), then it is recommended that you choose complex model (i.e InceptionResNetV2).")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Faster-rcnn models are generally more accurate than ssd models. However, sometimes you are better off using ssd models if your data is easy to learn (i.e 1 or 2 bounding box per image)."))),Object(r.b)("h4",{id:"frcnn-nas-coco"},"frcnn-nas-coco:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"If you are using ",Object(r.b)("inlineCode",{parentName:"li"},"frcnn-nas-coco"),", then please choose a machine with at least 2 GPUs as this model requires more memory. A machine with 1 GPU will throw an error.")),Object(r.b)("p",null,"This is a type of faster-rcnn model with NAS backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2)."),Object(r.b)("p",null,"For how to set epochs, you can take a look at first model since both models are faster-rcnn based."),Object(r.b)("p",null,"Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn't change batch_size."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},Object(r.b)("em",{parentName:"strong"},"Defaults")),": batch_size: 1, learning_rate: 0.0003, epochs=10000"),Object(r.b)("h4",{id:"frcnn-res101-coco"},"frcnn-res101-coco:"),Object(r.b)("p",null,"This is a type of faster-rcnn model with ResNet101 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). "),Object(r.b)("p",null,"For how to set epochs, you can take a look at first model since both models are faster-rcnn based."),Object(r.b)("p",null,"Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn't change batch_size."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},Object(r.b)("em",{parentName:"strong"},"Defaults")),": batch_size: 1, learning_rate: 0.0003, epochs=10000"),Object(r.b)("h4",{id:"frcnn-res101-lowp"},"frcnn-res101-lowp"),Object(r.b)("p",null,"This is a type of faster-rcnn model with ResNet101 backbone with low number of proposals. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). If you are looking for more complex and accurate model then check out frcnn-res101-coco or frcnn-inc-resv2-atr-coco."),Object(r.b)("p",null,"For how to set epochs, you can take a look at first model since both models are faster-rcnn based."),Object(r.b)("p",null,"Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn't change batch_size."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},Object(r.b)("em",{parentName:"strong"},"Defaults")),": batch_size: 1, learning_rate: 0.0003, epochs=10000"),Object(r.b)("h4",{id:"frcnn-res50-coco"},"frcnn-res50-coco"),Object(r.b)("p",null," This is a type of faster-rcnn model with ResNet50 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). If you are looking for more complex and accurate model then check out frcnn-res101-coco or frcnn-inc-resv2-atr-coco."),Object(r.b)("p",null,"For how to set epochs, you can take a look at first model since both models are faster-rcnn based."),Object(r.b)("p",null,"Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn't change batch_size."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},Object(r.b)("em",{parentName:"strong"},"Defaults")),": batch_size: 1, learning_rate: 0.0003, epochs=10000"),Object(r.b)("h4",{id:"ssd-mobilenet-v2-coco"},"ssd-mobilenet-v2-coco"),Object(r.b)("p",null,"SSD-based networks such as ",Object(r.b)("inlineCode",{parentName:"p"},"ssd-mobilenet-v2")," are faster than faster-rcnn based models. However, they are not as accurate as faster-rcnn based models. This model is generally recommended since its accurate and fast enough. If you don't know much about your data or the complexity of your data, then we recommend you go with this model."),Object(r.b)("p",null,"You will find the pre-trained model and config file for ssd-mobilenetv2 model trained on COCO dataset."),Object(r.b)("p",null,"This model is a good place to start if you don't have any specific model in mind. If you are data is very complicated (i.e many annotations per image) then you should prefer faster-rcnn models over ssd."),Object(r.b)("p",null,"Depending upon your data, you can set epochs to train your model. There is no standard value which can work for all datasets. You generally have to try different number of epochs to get the best model. Ideally, you do so by monitoring loss of your model while training. But if you are looking for a recommendation. Then, we recommend you set epochs as follows: (number of images / batch_size (default: 24)) * 1000. For instance, if you have 100 images, then your epochs will be 4000 (rounded). Please note that the model will be trained using a pre-trained model, so you don't need to train as long as you would have to when not using the pre-trained model."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},Object(r.b)("em",{parentName:"strong"},"Defaults")),": batch_size: 24, learning_rate: 0.004, epochs=10000"),Object(r.b)("p",null,"Please note that same instructions apply for ssd-mobilenet-v1 and ssd-mobilenetlite. The only difference is the backbone model (i.e mobilenet v1) that they use."),Object(r.b)("h2",{id:"training-maskrcnn-model-through-cvat"},"Training MaskRCNN model through CVAT"),Object(r.b)("p",null,"MaskRCNN is a popular model for segmentation tasks. We use ",Object(r.b)("a",Object(o.a)({parentName:"p"},{href:"https://github.com/matterport/Mask_RCNN"}),"this")," implementation of MaskRCNN for training and inference."),Object(r.b)("p",null,"The process to train a Mask-RCNN model on CVAT is similar to the above process except that you need to select Mask-RCNN after clicking on Create Annotation Model."),Object(r.b)("p",null,Object(r.b)("img",Object(o.a)({parentName:"p"},{src:"/img/maskrcnn-training.png",alt:"MaskRCNN Workflow"}))),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},Object(r.b)("em",{parentName:"strong"},"Parameters")),": Even though you don't need to enter any other parameters to start the training of Mask-RCNN, it is recommended that you pass correct epochs according your data. Mask-RCNN is a very deep model which takes too much time to train and also to get enough accuracy.\nWe allow you to set epochs for three different parts of the model. These parts are called ",Object(r.b)("inlineCode",{parentName:"p"},"stage1"),", ",Object(r.b)("inlineCode",{parentName:"p"},"stage2")," and ",Object(r.b)("inlineCode",{parentName:"p"},"stage3"),". You can set corresponding epochs as follows:"),Object(r.b)("pre",null,Object(r.b)("code",Object(o.a)({parentName:"pre"},{className:"language-bash"}),"stage-1-epochs=1\nstage-2-epochs=2\nstage-3-epochs=3\n")),Object(r.b)("p",null,"If you have few images (few hundreds), then we recommend you set total epochs (stage1+stage2+stage3) less than 10. We advise you set more epochs for stage1 than others. As your data size increases or the complexity of your data increases you might want to increase epochs. "),Object(r.b)("p",null,"If you have ~1000 images then you don't have to set any parameters, CVAT will take care of it."),Object(r.b)("h2",{id:"how-to-run-inference-on-test-data-using-trained-model"},"How to run inference on test data using trained model"),Object(r.b)("p",null,"Often you are required to make prediction on test data. Using CVAT on Onepanel, you can easily train/test your model and visualize output. Once you have the trained model, upload it to the CVAT by clicking on ",Object(r.b)("inlineCode",{parentName:"p"},"Create new model")," on ",Object(r.b)("inlineCode",{parentName:"p"},"models")," tab."),Object(r.b)("p",null,"Now, create a task with your test data."),Object(r.b)("p",null,"Click on Actions for that task and select Automatic annotation. Select the model you just uploaded and hit submit. It will run the inference using the model you selected. Below is a sample frame whose output was generated using the trained model. For more information on automatic annotation, please refer to our guide on ",Object(r.b)("a",Object(o.a)({parentName:"p"},{href:"./cvat_automatic_annotation"}),"automatic annotation"),"."),Object(r.b)("p",null,Object(r.b)("img",Object(o.a)({parentName:"p"},{src:"/img/inference_output.PNG",alt:"Inference Output"}))),Object(r.b)("h2",{id:"notes"},"Notes"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"There are certain parameters that are prefixed with ",Object(r.b)("inlineCode",{parentName:"li"},"cvat")," in TF Object Detection Training and MaskRCNN Training workflows. Those are special parameters and will be populated in whole or partly by the CVAT. For example, ",Object(r.b)("inlineCode",{parentName:"li"},"cvat-output-path")," is generated by the CVAT and it won't be shown to users. Another example is ",Object(r.b)("inlineCode",{parentName:"li"},"cvat-finetune-checkpoint"),". CVAT will automatically find all available checkpoints for a given workflow/model since they are available locally because of file syncer. "),Object(r.b)("li",{parentName:"ul"},"Please note that these instructions are for default models that we provide. You can always edit these workflows or even add your own workflows/models and train them."),Object(r.b)("li",{parentName:"ul"},"You  can find the code that connects  ",Object(r.b)("a",Object(o.a)({parentName:"li"},{href:"https://github.com/onepanelio/cvat-training"}),"here"),".")))}u.isMDXComponent=!0},156:function(e,t,n){"use strict";n.d(t,"a",(function(){return b})),n.d(t,"b",(function(){return p}));var o=n(0),a=n.n(o);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=a.a.createContext({}),u=function(e){var t=a.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},b=function(e){var t=u(e.components);return a.a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.a.createElement(a.a.Fragment,{},t)}},m=a.a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,i=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),b=u(n),m=o,p=b["".concat(i,".").concat(m)]||b[m]||d[m]||r;return n?a.a.createElement(p,s(s({ref:t},l),{},{components:n})):a.a.createElement(p,s({ref:t},l))}));function p(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var l=2;l<r;l++)i[l]=n[l];return a.a.createElement.apply(null,i)}return a.a.createElement.apply(null,n)}m.displayName="MDXCreateElement"}}]);