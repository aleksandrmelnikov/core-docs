<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="generator" content="Docusaurus v2.0.0-alpha.56">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-106005416-5","auto"),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="search" type="application/opensearchdescription+xml" title="Onepanel CE" href="/opensearch.xml"><title data-react-helmet="true">Creating annotation models on CVAT | Onepanel CE</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Creating annotation models on CVAT | Onepanel CE"><meta data-react-helmet="true" name="description" content="Why pre-annotate?"><meta data-react-helmet="true" property="og:description" content="Why pre-annotate?"><meta data-react-helmet="true" property="og:url" content="https://docs.onepanel.ai/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.png"><link data-react-helmet="true" rel="canonical" href="https://docs.onepanel.ai/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model"><link rel="stylesheet" href="/styles.d4a9fa70.css">
<link rel="preload" href="/styles.74672a45.js" as="script">
<link rel="preload" href="/runtime~main.4d627c70.js" as="script">
<link rel="preload" href="/main.89dca43b.js" as="script">
<link rel="preload" href="/1.9016d2e5.js" as="script">
<link rel="preload" href="/2.8775ef2f.js" as="script">
<link rel="preload" href="/3.1171db3e.js" as="script">
<link rel="preload" href="/1be78505.65cc56fe.js" as="script">
<link rel="preload" href="/20ac7829.b902d665.js" as="script">
<link rel="preload" href="/aeca0258.8d94eedd.js" as="script">
</head>
<body>
<div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img class="navbar__logo" src="/img/onepanel-logo-white.svg" alt="Onepanel logo"><strong class="navbar__title"></strong></a><a class="navbar__item navbar__link" href="/docs/getting-started/quickstart">Getting Started</a><a class="navbar__item navbar__link" href="/docs/deployment/overview">Deploy and Manage</a><a class="navbar__item navbar__link" href="/docs/reference/overview">Reference</a><a class="navbar__item navbar__link" href="/docs/api-sdk/overview">APIs and SDKs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/onepanelio/core" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a><div class="navbar__search"><div class="searchWrapper_-F3-"><span aria-label="expand searchbar" role="button" class="searchIconButton_1Dnz" tabindex="0"></span><input type="search" id="search_input_react" placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_2eto"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img class="navbar__logo" src="/img/onepanel-logo-white.svg" alt="Onepanel logo"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/getting-started/quickstart">Getting Started</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/deployment/overview">Deploy and Manage</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/reference/overview">Reference</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/api-sdk/overview">APIs and SDKs</a></li><li class="menu__list-item"><a href="https://github.com/onepanelio/core" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_1kjD"><div class="docSidebarContainer_1cYp"><div class="sidebar_1kLs"><div class="menu menu--responsive menu_w2sC"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_2vk4" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/getting-started/quickstart">Quick start</a></li><li class="menu__list-item"><a class="menu__link" href="#!">Concepts</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/concepts/namespaces">Namespaces</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/concepts/workspaces">Workspaces</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/concepts/workflows">Workflows</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/concepts/environment-variables">Environment variables</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="#!">Use cases</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="#!" tabindex="0">Computer vision</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="#!" tabindex="0">Image/video auto annotation</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_quick_guide">Quick start</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_automatic_annotation">Automatic annotation</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model">Create annotation model</a></li></ul></li></ul></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/docs/getting-started/contributing">Contributing</a></li></ul></div></div></div><main class="docMainContainer_FFX1"><div class="container padding-vert--lg docItemWrapper_1cc7"><div class="row"><div class="col docItemCol_2GOA"><div class="docItemContainer_2cwg"><article><header><h1 class="docTitle_1vWb">Creating annotation models on CVAT</h1></header><div class="markdown"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="why-pre-annotate"></a>Why pre-annotate?<a aria-hidden="true" tabindex="-1" class="hash-link" href="#why-pre-annotate" title="Direct link to heading">#</a></h2><p>Pre-annotation will cut the time to annotate large amounts of data by orders of magnitude. The idea is simple, annotate once then QC each successive dataset after.</p><p>Once you have annotated enough data, you can train a model to pre-annotate the rest of your images with a few button clicks.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="training-object-detection-model-through-cvat"></a>Training object detection model through CVAT<a aria-hidden="true" tabindex="-1" class="hash-link" href="#training-object-detection-model-through-cvat" title="Direct link to heading">#</a></h2><ol><li>Annotate enough images in your CVAT task.  </li><li>Go back to your CVAT dashboard and click on <code>Create New Annotation Model</code> in that task. You will see a popup with a few options.  </li><li>Select the appropriate model type (TensorFlow OD API recommended) and then select the model (i.e ssd-mobilenet-v2-coco-201).  </li><li>Select the machine type. A machine with multiple GPUs will speed up your training process.    </li><li>Enter optional arguments. See below for more details.  </li><li>Click on link to new model in email that will be sent to you once model training completes - locate the tf_annoation_model folder and inspect the contents.  </li><li>Mount this new dataset to the CVAT workspace and click the button for the Model manager.  Then select files in tf_annotation_model folder.  </li><li>Click TF_Annotion button for the current task.  </li></ol><p><img src="/img/auto-annotation-v.2.0.png" alt="CVAT flowchart"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="arguments-optional"></a>Arguments (optional)<a aria-hidden="true" tabindex="-1" class="hash-link" href="#arguments-optional" title="Direct link to heading">#</a></h2><p>You can optionally specify some arguments in the <code>Arguments</code> field seperated by <code>;</code>. </p><p>Here is a sample: <code>epochs=100;batch_size=24</code>. </p><ul><li>epochs : number of epochs to train your model for. By default, we will train for an appropriate number of epochs depending upon the model.</li><li>batch_size : batch size for the training</li><li>initial_learning_rate : initial learning rate for the model. We recommend you do not change this.</li><li>num_clones (default=1): number of GPUs to train the modelÂ </li></ul><p>If you select a Machine type with 4 GPUs (Tesla V100), the following command can be used:
<code>epochs=300000;num_clones=4</code></p><ul><li>Note that num_clones is 4 because there are 4 GPUs available.</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="choosing-the-right-base-model"></a>Choosing the right base model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#choosing-the-right-base-model" title="Direct link to heading">#</a></h2><p>You can use any of the models that we support to train your custom pre-annotation models. Here, we provide a brief explanation on how to choose one model over another based on your needs. Some models are faster than others, whereas some are more accurate than others.  We hope this information will help you choose the right model for your task. </p><ul><li><p>Note that we don&#x27;t support Yolo and MaskRCNN models yet.</p></li><li><p>We currently support a few faster-rcnn models. All of these models are similar except that of the backbone used for the feature extraction. The backbones used are, in increasing order of complexity (i.e more layers), ResNet50, ResNet101, InceptionResNetV2. As the model complexity increases the computation requirement will also increase. If you have very complicated data (i.e hundreds of annotations in one image), then it is recommended that you choose complex model (i.e InceptionResNetV2).</p></li><li><p>Faster-rcnn models are generally more accurate than ssd models. However, sometimes you are better off using ssd models if your data is easy to learn (i.e 1 or 2 bounding box per image).</p></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="frcnn-inc-resv2-atr-coco"></a>frcnn-inc-resv2-atr-coco:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-inc-resv2-atr-coco" title="Direct link to heading">#</a></h3><p>This is a type of faster-rcnn model with InceptionResNetV2 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2).</p><p>Depending upon your data, you can set epochs to train your model. There is no standard value which can work for all datasets. You generally have to try different number of epochs to get the best model. Ideally, you do so by monitoring loss of your model while training. But if you are looking for a recommendation. Then, we recommend you set epochs as follows: (number of images / batch_size (default: 1)) * 500. For instance, if you have 100 images, then your epochs will be 50000(rounded). Please note that the model will be trained using a pre-trained model, so you don&#x27;t need to train as long as you would have to when not using the pre-trained model.</p><p>Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change batch_size.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><p><strong><em>Model</em></strong>: <a href="https://c.onepanel.io/onepanel-demo/datasets/frcnn-inc-resv2-atr-coco/details" target="_blank" rel="noopener noreferrer">https://c.onepanel.io/onepanel-demo/datasets/frcnn-inc-resv2-atr-coco/details</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="frcnn-nas-coco"></a>frcnn-nas-coco:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-nas-coco" title="Direct link to heading">#</a></h3><ul><li>If you are using <code>frcnn-nas-coco</code>, then please choose a machine with at least 2 GPUs as this model requires more memory. A machine with 1 GPU will throw an error.</li></ul><p>This is a type of faster-rcnn model with NAS backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2).</p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change batch_size.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><p><strong><em>Model</em></strong>: <a href="https://c.onepanel.io/onepanel-demo/datasets/frcnn-nas-coco/details" target="_blank" rel="noopener noreferrer">https://c.onepanel.io/onepanel-demo/datasets/frcnn-nas-coco/details</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="frcnn-res101-coco"></a>frcnn-res101-coco:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-res101-coco" title="Direct link to heading">#</a></h3><p>This is a type of faster-rcnn model with ResNet101 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). </p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change batch_size.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><p><strong><em>Model</em></strong>: <a href="https://c.onepanel.io/onepanel-demo/datasets/frcnn-res101-coco/details" target="_blank" rel="noopener noreferrer">https://c.onepanel.io/onepanel-demo/datasets/frcnn-res101-coco/details</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="frcnn-res50-coco"></a>frcnn-res50-coco<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-res50-coco" title="Direct link to heading">#</a></h3><p> This is a type of faster-rcnn model with ResNet50 backbone. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). If you are looking for more complex and accurate model then check out frcnn-res101-coco or frcnn-inc-resv2-atr-coco.</p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change batch_size.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><p><strong><em>Model</em></strong>: <a href="https://c.onepanel.io/onepanel-demo/datasets/frcnn-res50-lowp/details" target="_blank" rel="noopener noreferrer">https://c.onepanel.io/onepanel-demo/datasets/frcnn-res50-lowp/details</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="frcnn-res50-lowp"></a>frcnn-res50-lowp<a aria-hidden="true" tabindex="-1" class="hash-link" href="#frcnn-res50-lowp" title="Direct link to heading">#</a></h3><p>This is a type of faster-rcnn model with ResNet50 backbone with low number of proposals. If you are not sure about which model to use then we recommend you use SSD based model (i.e ssd-mobilenet-v2). If you are looking for more complex and accurate model then check out frcnn-res101-coco or frcnn-inc-resv2-atr-coco.</p><p>For how to set epochs, you can take a look at first model since both models are faster-rcnn based.</p><p>Please note that current implementation of faster-rcnn inTensorFlow Object Detection API does not support batch training. That is, you shouldn&#x27;t change batch_size.</p><p><strong><em>Defaults</em></strong>: batch_size: 1, learning_rate: 0.0003, epochs=10000</p><p><strong><em>Model</em></strong>: <a href="https://c.onepanel.io/onepanel-demo/datasets/frcnn-res50-coco/details" target="_blank" rel="noopener noreferrer">https://c.onepanel.io/onepanel-demo/datasets/frcnn-res50-coco/details</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="ssd-mobilenet-v2-coco"></a>ssd-mobilenet-v2-coco:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#ssd-mobilenet-v2-coco" title="Direct link to heading">#</a></h3><p>SSD-based networks such as <code>ssd-mobilenet-v2</code> are faster than faster-rcnn based models. However, they are not as accurate as faster-rcnn based models. This model is generally recommended since its accurate and fast enough. If you don&#x27;t know much about your data or the complexity of your data, then we recommend you go with this model.</p><p>You will find the pre-trained model and config file for ssd-mobilenetv2 model trained on COCO dataset.</p><p>This model is a good place to start if you don&#x27;t have any specific model in mind. If you are data is very complicated (i.e many annotations per image) then you should prefer faster-rcnn models over ssd.</p><p>Depending upon your data, you can set epochs to train your model. There is no standard value which can work for all datasets. You generally have to try different number of epochs to get the best model. Ideally, you do so by monitoring loss of your model while training. But if you are looking for a recommendation. Then, we recommend you set epochs as follows: (number of images / batch_size (default: 24)) * 1000. For instance, if you have 100 images, then your epochs will be 4000 (rounded). Please note that the model will be trained using a pre-trained model, so you don&#x27;t need to train as long as you would have to when not using the pre-trained model.</p><p><strong><em>Defaults</em></strong>: batch_size: 24, learning_rate: 0.004, epochs=15000</p><p><strong><em>Model</em></strong>: <a href="https://c.onepanel.io/onepanel-demo/datasets/ssd-mobilenet-v2-coco/details" target="_blank" rel="noopener noreferrer">https://c.onepanel.io/onepanel-demo/datasets/ssd-mobilenet-v2-coco/details</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="training-segmentation-model-through-cvat"></a>Training segmentation model through CVAT<a aria-hidden="true" tabindex="-1" class="hash-link" href="#training-segmentation-model-through-cvat" title="Direct link to heading">#</a></h2><p>The process to train a Mask-RCNN model on CVAT is similar to the above process except that you need to select Mask-RCNN after clicking on Create Annotation Model.
<strong><em>Parameters</em></strong>: Even though you don&#x27;t need to enter any parameters to start the training of Mask-RCNN, it is recommended that you pass correct epochs according your data. Mask-RCNN is a very deep model which takes too much time to train and also to get enough accuracy.
We allow you to set epochs for three different parts of the model. These parts are called <code>stage1</code>, <code>stage2</code> and <code>stage3</code>. You can set corresponding epochs through <code>--stage1_epochs</code>, <code>--stage2_epochs</code>, and <code>--stage3_epochs</code>.</p><p>If you have few images (few hundreds), then we recommend you set total epochs (stage1+stage2+stage3) less than 10. We advise you set more epochs for stage1 than others. As your data size increases or the complexity of your data increases you might want to increase epochs. </p><p>If you have ~1000 images then you don&#x27;t have to set any parameters, CVAT will take care of it.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="adding-your-own-base-model-to-cvat"></a>Adding your own base model to CVAT<a aria-hidden="true" tabindex="-1" class="hash-link" href="#adding-your-own-base-model-to-cvat" title="Direct link to heading">#</a></h2><p>You can also add your own base models to CVAT via Onepanel. Â Please email us at <a href="mailto:info@onepanel.io">info@onepanel.io</a> to learn how.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_ZqCz" id="open-source-code"></a>Open source code<a aria-hidden="true" tabindex="-1" class="hash-link" href="#open-source-code" title="Direct link to heading">#</a></h2><p>You can find the code that triggers dataset creation, base model pulling, model training, and model conversion here:</p><p><a href="https://github.com/onepanelio/cvat-training" target="_blank" rel="noopener noreferrer">https://github.com/onepanelio/cvat-training</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/onepanelio/core-docs/tree/master/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_annotation_model.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/getting-started/use-cases/computervision/annotation/cvat/cvat_automatic_annotation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Semi-autmatic annotation with CVAT</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/getting-started/contributing"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Contributing Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_TbNY"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-pre-annotate" class="table-of-contents__link">Why pre-annotate?</a></li><li><a href="#training-object-detection-model-through-cvat" class="table-of-contents__link">Training object detection model through CVAT</a></li><li><a href="#arguments-optional" class="table-of-contents__link">Arguments (optional)</a></li><li><a href="#choosing-the-right-base-model" class="table-of-contents__link">Choosing the right base model</a><ul><li><a href="#frcnn-inc-resv2-atr-coco" class="table-of-contents__link">frcnn-inc-resv2-atr-coco:</a></li><li><a href="#frcnn-nas-coco" class="table-of-contents__link">frcnn-nas-coco:</a></li><li><a href="#frcnn-res101-coco" class="table-of-contents__link">frcnn-res101-coco:</a></li><li><a href="#frcnn-res50-coco" class="table-of-contents__link">frcnn-res50-coco</a></li><li><a href="#frcnn-res50-lowp" class="table-of-contents__link">frcnn-res50-lowp</a></li><li><a href="#ssd-mobilenet-v2-coco" class="table-of-contents__link">ssd-mobilenet-v2-coco:</a></li></ul></li><li><a href="#training-segmentation-model-through-cvat" class="table-of-contents__link">Training segmentation model through CVAT</a></li><li><a href="#adding-your-own-base-model-to-cvat" class="table-of-contents__link">Adding your own base model to CVAT</a></li><li><a href="#open-source-code" class="table-of-contents__link">Open source code</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Onepanel, Inc.</div></div></div></footer></div>
<script src="/styles.74672a45.js"></script>
<script src="/runtime~main.4d627c70.js"></script>
<script src="/main.89dca43b.js"></script>
<script src="/1.9016d2e5.js"></script>
<script src="/2.8775ef2f.js"></script>
<script src="/3.1171db3e.js"></script>
<script src="/1be78505.65cc56fe.js"></script>
<script src="/20ac7829.b902d665.js"></script>
<script src="/aeca0258.8d94eedd.js"></script>
</body>
</html>